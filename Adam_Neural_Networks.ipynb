{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">NEURAL NETWORK OPTIMISATION: ADAM & MORE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Raphael Mikati, MSc General Engineering - ISAE Supaero (France) & MSc Management and Strategy - London School of Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Pre-requisite & preliminary elements](#sec1)\n",
    "    1. [Why do you need optimisation algorithms in neural networks?](#sec1-1)\n",
    "    2. [Gradient descent, learning rate, Adagrad and RMSProp](#sec1-2)\n",
    "    3. [What is a \"moment\"?](#sec1-3)\n",
    "2. [How does ADAM work?](#sec2)\n",
    "    1. [Motivation & idea](#sec2-1)\n",
    "    2. [The ADAM procedure](#sec2-2)\n",
    "    3. [Description of the algorithm](#sec2-3)\n",
    "3. [Python implementation & simulations](#sec3)\n",
    "4. [Advantages & convergence issues](#sec4)\n",
    "    1. [General remarks about ADAM's strenghts and weaknesses](#sec4-1)\n",
    "    2. [On ADAM's convergence issues](#sec4-2)\n",
    "5. [A few words on ADAMAX](#sec5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM is an optimisation algorithm specifically tailored for training deep neural networks. It was first published in 2014 (Kingma and Lei Ba (2014)) and presented at the ICLR 2015 conference for deep learning practitioners. Numerous are the optimisation algorithms that work well in few problems but fail to generalise properly to the wide range of neural networks you might want to train. Indeed, the choice of the optimisation algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days. \n",
    "\n",
    "The name ADAM is derived from \"Adaptive Moment Estimation\". We'll see later on how the mathematical concept of moment comes into play.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec1\"></a> 1. PREREQUISITES & PRELIMINARY ELEMENTS\n",
    "\n",
    "Before we start off, let us make a quick recap of some useful and essential elements to understand the ADAM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1-1\"></a> 1.1. Why do you need optimisation algorithms in neural networks? \n",
    "Remember that when building neural networks, we need to minimise a certain loss function so as to learn the weights of our network. The algorithms that are most used for this purpose are part of the gradient descent family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1-2\"></a>1.2. Gradient descent optimisation, learning rates and RMSProp, AdaGrad\n",
    "\n",
    "The gradient descent family is one of the most popular categories of algorithms to perform optimisation tasks and by far the most common one in deep learning.\n",
    "\n",
    "In a nutshell, gradient descent algorithms allow you to minimise an objective function $J(\\theta)$, parametrised by a vector of parameters $\\theta \\in \\mathbf{R}^d$. It updates the parameter in the opposite direction of the gradient of the objective function $\\nabla_\\theta J(\\theta)$ with respect to the parameters.\n",
    "\n",
    "The **learning rate**, $\\alpha$, determines the size of the steps taken to reach a (local) minimum.\n",
    "\n",
    "There are three variants of gradient descent which differ in the number of data points used to calculate the $\\nabla_\\theta J(\\theta)$:\n",
    "\n",
    "- **Batch gradient descent:** it calculates $\\nabla_\\theta J(\\theta)$ for the entire training set:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\nabla_\\theta J(\\theta) $$\n",
    "\n",
    "\n",
    "- **Stochastic gradient descent (SGD):** it performs a parameter update for each training example $x_i$ and label $y_i$:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\nabla_\\theta J(\\theta; x_i; y_i) $$\n",
    "\n",
    "\n",
    "- **Mini-batch gradient descent:** it is a combination of the batch gradient descent and SGD since it performs an update for every mini-batch of n training data:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\nabla_\\theta J(\\theta; x_{(i:i+n)}; y_{(i:i+n)}) $$\n",
    "\n",
    "\n",
    "Now, we need to have a basic understanding of two gradient descent algorithms: Adagrad and RMSProp. \n",
    "\n",
    "- **Adagrad:** Adagrad is an algorithm for gradient-based optimization that adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.\n",
    "\n",
    "\n",
    "- **RMSProp (Root Mean Square Propagation):** it was developed by Hinton and Tieleman (2012) and addresses the need to resolve Adagrad's radically diminishing learning rates. So as to understand this need, let us take an example in which we would like to optimise a cost function which has contours like below. The red dot denotes the position of the local minimum.\n",
    "\n",
    "<img src=\"rms1.png\" width=\"800px\"></img>\n",
    "\n",
    "We start our gradient descent from point ‘A’ and after one iteration we may end up at point ‘B’, the other side of the ellipse. Then another step of gradient descent may end up at point ‘C’. With each iteration of gradient descent, we move towards the local optima with up and down oscillations. If we use larger learning rate then the vertical oscillation will have higher magnitude. So, this vertical oscillation slows down our gradient descent and prevents us from using a much larger learning rate.\n",
    "\n",
    "<img src=\"rms2.png\" width=\"800px\"></img>\n",
    "\n",
    "Thus, we want to slow down the learning rate in the vertical direction and speed up (or at least not slow it down) in the horizontal direction. This is what the RMSProp procedure does. \n",
    "\n",
    "We'll dive into the mathematical formulation of the Adagrad and RMSProp procedures later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sec1-3\"></a> 1.3. What is a \"moment\"?\n",
    "Mathematically, the n-th moment $m_n$ of a random variable X is defined as the expected value of that variable to the power of n:\n",
    "\n",
    "$$m_n = \\displaystyle \\mathbb{E}[X^n]$$\n",
    "\n",
    "We can now understand that the expectation of a given random variable represents its first moment. If we consider the random variables X and Z defined as follows:\n",
    "$$Z = X - \\mathbb{E}[X]$$\n",
    "We can see that the second moment of Z is the variance of X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec2\"></a> 2. HOW DOES ADAM WORK?\n",
    "### <a id=\"sec2-1\"></a> 2.1. Motivation & idea\n",
    "ADAM is an optimisation algorithm that can be used instead of the classical stochastic gradient descent (SGD) procedure to update the network weights in a given neural network. In concrete terms, it optimises stochastic objective functions. The classical SGD maintains a constant learning rate for all weight updates and the learning rate does not change during training. ADAM proposes to combine the advantages of Adagrad which works well with sparse gradients and RMSProp which maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "**But what could a stochastic objective function be?** Stochastic function can be a functions composed of a sum of subfunctions evaluated at different subsamples of data or more generally noisy functions.\n",
    "\n",
    "\n",
    "### <a id=\"sec2-2\"></a> 2.2. The ADAM procedure\n",
    "Let us dive into the details of the ADAM algorithm.\n",
    "\n",
    "Let $f(\\theta)$ be a stochastic objective function. We assumed that it is differentiable with respect to its parameters $\\theta$. Let $g_{t,i} = \\nabla_\\theta f_{t}(\\theta_{t,i})$ denote the gradient of our objective function (i.e. the vector of partial derivatives of $f_t$ with respect to $\\theta_i$ evaluated at timestep t). We wish to minimise the expected value of the function f: $\\mathbb{E}[f(\\theta)]$ (which the first moment of the random variable or stochastic function f).\n",
    "\n",
    "\n",
    "First of all, we need to exhibit the parameters update rule of Adagrad and RMSProp. As we have previsouly seen, in a gradient descent scheme, parameters get updated.\n",
    "\n",
    "- **Adagrad's update rule:**\n",
    "\n",
    "$$\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii} + \\epsilon}} g_{t,i} $$\n",
    "\n",
    "where $G_t \\in \\mathbf{R}^{dxd}$ is a diagonal matrix where each diagonal element is the sum of the squares of the gradients with respect to $\\theta_i$ up to time step t, while $\\epsilon$ is a smoothing term that avoids division by zero.\n",
    "\n",
    "\n",
    "\n",
    "- **RMSProp's update rule:**\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\mathbb{E}[g^{2}_{t}] + \\epsilon}} g_{t} $$\n",
    "\n",
    "with $\\mathbb{E}[g^{2}_{t}] = \\gamma \\mathbb{E}[g^{2}_{t-1}] + (1 - \\gamma)g_t$ (usually $\\gamma = 0.9$)\n",
    "\n",
    "$g_{t}$ is equal to $\\nabla \\theta f_{t}(\\theta_{t})$ and $\\mathbb{E}[g^{2}_{t}]$ is the second moment of the random variable g at timestep t. (g can be considered a random variable since it is the gradient of f, a stochastic function).\n",
    "\n",
    "\n",
    "\n",
    "- **ADAM's update rule:** \n",
    "\n",
    "Let us build the ADAM's update rule step by step. The idea of the procedure is to update the parameter $\\theta$ using the squared gradients ($g_{t}^{2}$ through $v_t$) to scale the learning rate ($\\alpha$) like RMSProp and making use of the exponential moving average of the gradient ($m_t$) instead of the gradient itself like Adagrad for instance. Some constants $\\beta_1$ and $\\beta_2$ are hyper-parameters used to control the exponential decay rates of $m_t$ and $v_t$. \n",
    "\n",
    "$$m_t = (1-\\beta_1) \\sum\\limits_{i=1}^t \\beta_{1}^{t-i} g_i$$\n",
    "$$v_t = (1-\\beta_2) \\sum\\limits_{i=1}^t \\beta_{2}^{t-i} g_{i}^{2}$$\n",
    "\n",
    "The algorithm updates $m_t$ and $v_t$ as it goes along using the following relations:\n",
    "$$m_{t+1} = \\beta_{1} m_{t} + (1 - \\beta_{1})g_t $$\n",
    "$$v_{t+1} = \\beta_{2} v_{t} + (1 - \\beta_{2})g^{2}_t $$\n",
    "\n",
    "So far, so good, isn't it? Well almost, there is a slight issue: $m_t$ and $v_t$ are estimators respectively of the gradient and the squared gradient. In the ADAM algorithm, we need to initialise their values. And as we do so by initialising these moving averages with zeros, this gives us biased estimators. Even so, we still want to manipulate unbiased estimators i.e. we wish the following properties to hold true:\n",
    "$$ \\mathbb{E}[m_{t}] = \\mathbb{E}[g_{t}]$$\n",
    "$$ \\mathbb{E}[v_{t}] = \\mathbb{E}[g^{2}_{t}]$$\n",
    "As we can see, the notion of \"moments\" comes into play (hence ADAM: Adaptive Moment Estimation)\n",
    "\n",
    "This is thus why, we need to correct the estimators $m_t$ and $v_t$ (bias corrections) to get $\\hat{m}_t$ and $\\hat{v}_t$, respectively the unbiased estimators of the gradient and the squared gradient ($\\beta_{i}^{t}$ denotes $\\beta_i$ to the power t).\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta^{t}_1} $$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1-\\beta^{t}_2} $$\n",
    "\n",
    "Thus, an initialisation of moving averages with zeros is compatible with getting unbiased estimators thanks to this bias correction trick. This proves very convenient in the algorithm.\n",
    "\n",
    "We can now exhibit the ADAM's update rule for the parameters:\n",
    "\n",
    "$$\\boxed{\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t}} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <a id=\"sec2-3\"></a> 2.3. Description of the algorithm\n",
    "\n",
    "<img src=\"adam.png\" width=\"800px\"></img>\n",
    "\n",
    "Kingma and Lei Ba (2014), the inventors of ADAM recommends to use the value 0.001 for $\\alpha$, 0.9 for $\\beta_1$, 0.999 for $\\beta_2$ and $10^{-8}$ for $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3\"></a> 3. PYTHON IMPLEMENTATION & SIMULATIONS\n",
    "\n",
    "We propose a Python implementation of the algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters' settings\n",
    "alpha = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The objective function\n",
    "def objective_function (x):\n",
    "    return(x*x - 10*x + 4)\n",
    "\n",
    "# The gradient of the objective function under study\n",
    "def gradient_obj_func (x):\n",
    "    return(2*x - 10)\n",
    "\n",
    "# Let us plot the objective function\n",
    "x = np.linspace(-35,35,100)\n",
    "y = objective_function(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.title('Our Objective Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global minimum of this objective function is located at (x = 5, y = -21).\n",
    "\n",
    "Now, it's your turn: could you try to code a function implementing the ADAM algorithm. Give it a go! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ADAM algorithm\n",
    "\n",
    "def ADAM (grad_func, alpha, beta_1, beta_2, epsilon): # grad_func denotes the gradient of our objective function\n",
    "    # Initialisation \n",
    "    theta_0 = 0\n",
    "    m_t = 0\n",
    "    v_t = 0\n",
    "    t = 0\n",
    "    theta_0_prev = -1.0 #arbitrary value just to initialise the variable\n",
    "    \n",
    "    # Diving into the ADAM procedure\n",
    "    while (theta_0 != theta_0_prev):\n",
    "        t+=1\n",
    "        g_t = grad_func(theta_0) # gradient calculation\n",
    "        m_t = beta_1*m_t + (1.0-beta_1)*g_t # 1st moment estimate\n",
    "        v_t = beta_2*v_t + (1.0-beta_2)*(g_t*g_t) # 2nd moment estimate\n",
    "        m_hat = m_t/(1.0-(beta_1**t)) # 1st moment bias correction\n",
    "        v_hat = v_t/(1.0-(beta_2**t)) # 2nd moment bias correction\n",
    "        theta_0_prev = theta_0\n",
    "        theta_0 = theta_0 - (alpha*m_hat)/(math.sqrt(v_hat)+epsilon) # update\n",
    "    return(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999999999979\n"
     ]
    }
   ],
   "source": [
    "print(ADAM (gradient_obj_func, alpha, beta_1, beta_2, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec4\"></a> 4. ADVANTAGES & CONVERGENCE ISSUES\n",
    "### <a id=\"sec4-1\"></a> 4.1. General remarks about ADAM's strenghts and weaknesses\n",
    "When introducing the algorithm, Kingma and Lei Ba (2014) list the attractive benefits of using Adam on non-convex optimization problems:\n",
    "\n",
    "- Straightforward to implement\n",
    "- Computationally efficient\n",
    "- Little memory requirements\n",
    "- Invariant to diagonal rescale of the gradients\n",
    "- Well suited for problems that are large in terms of data and/or parameters\n",
    "- Appropriate for non-stationary objectives\n",
    "- Appropriate for problems with very noisy/or sparse gradients\n",
    "- Hyper-parameters have intuitive interpretation and typically require little tuning\n",
    "\n",
    "\n",
    "Since the publication of Kingma and Lei Ba's paper (2014), some research has been done to assess ADAM's performance. It appears that in some areas, ADAM does not converge to an optimal solution (e.g. for image classification (CIFAR data set) or certain funtions, we'll see one of them later on) whereas SGD with momentum algorithms do (momentum is a method consisting of adding fractions of previous gradients to the current one). Furthermore, Keskar and Socher (2017) have shown that by switching to SGD during training, they have managed to get better generalisation power than when using ADAM alone. More generally, Keskar and Socher (2017) have found that in earlier stages of training ADAM outperforms SGD but later the learning saturates. They have thus proposed a fix called \"SWATS\" for that. It consists of starting training the neural network with ADAM and then switching to SGD after a certain threshold. \n",
    "\n",
    "\n",
    "### <a id=\"sec4-2\"></a> 4.2. On ADAM's convergence issues\n",
    "Reddi et al. (2018) have presented an example of function for which ADAM fails to converge:\n",
    "\n",
    "$$f_{t}(x) = \\begin{cases} rx, & \\mbox{for } t\\mbox{ mod 3 = 1} \\\\ -x, & \\mbox{otherwise } \\end{cases} $$\n",
    "\n",
    "with $r > 2$ and $x \\in [-1, 1]$\n",
    "\n",
    "The optimum of this function is $x = - 1$, however ADAM seems unstable and converges to the value $x = 1$ for certain values of its hyper-parameters. A classical implementation of a SGD algorithm leads to an optimum of $x = -1$.\n",
    "\n",
    "Let us see this with the following Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of our objective function\n",
    "def gradient_f (x, t, r):\n",
    "    if t%3 == 1:\n",
    "        return (r)\n",
    "    else :\n",
    "        return(-1.0)\n",
    "\n",
    "# We are interested in values lying between [-1, 1]\n",
    "def projection (x):\n",
    "    if -1. <= x <= 1.:\n",
    "        return (x)\n",
    "    elif x <= -1.:\n",
    "        return (-1.0)\n",
    "    else:\n",
    "        return (1.0)        \n",
    "\n",
    "# Parameters of our objective function\n",
    "r = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a simple SGD algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(Niter, alpha): # alpha is the learning rate\n",
    "    x = 0 # initialisation of x\n",
    "    for t in range(Niter):\n",
    "        g_t = gradient_f(x, t, r)\n",
    "        x = projection(x-alpha*g_t)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take $\\alpha = 0.001$ and see what a SGD algorithm finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.998\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "Niter = 10000\n",
    "\n",
    "print(SGD(Niter,alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this implementation of the SGD algorithm converges towards the genuine minimum of our objective function. Let us see how ADAM performs if we use different hyper-parameters (different from the one tuned by Kingma and Lei Ba (2014)). We just need to tweak a little bit our ADAM algorithm coded previously to adapt to our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ADAM_bis (grad_func, Niter, alpha, beta_1, beta_2, epsilon): # grad_func denotes the gradient of our objective function\n",
    "    # Initialisation \n",
    "    x = 0\n",
    "    m_t = 0\n",
    "    v_t = 0\n",
    "    v_hat = 0\n",
    "    t = 0\n",
    "    x_prev = -10. #arbitrary value just to initialise the variable\n",
    "    \n",
    "    # Diving into the ADAM procedure\n",
    "    for t in range(Niter):\n",
    "        t+=1\n",
    "        g_t = grad_func(x, t, r) # gradient calculation\n",
    "        m_t = beta_1*m_t + (1.0-beta_1)*g_t # 1st moment estimate\n",
    "        v_t = beta_2*v_t + (1.0-beta_2)*(g_t*g_t) # 2nd moment estimate\n",
    "        m_hat = m_t/(1.0-(beta_1**t)) # 1st moment bias correction\n",
    "        v_hat = v_t/(1.0-(beta_2**t)) # 2nd moment bias correction\n",
    "        x_prev = x\n",
    "        x = projection(x - (alpha*m_hat)/(math.sqrt(v_hat)+epsilon)) # update\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989950631672736\n"
     ]
    }
   ],
   "source": [
    "# Setting up the hyperparameters\n",
    "beta_1 = 0.\n",
    "beta_2 = float(1/(1+r*r))\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Testing ADAM on our example\n",
    "print(ADAM_bis(gradient_f, Niter, alpha, beta_1, beta_2, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADAM algorithm does not converges to -1 if we decide to choose values that are different from the ones tuned by Kingam and Lei Ba (2014). It roughly gives the value 1. We can explain such an outcome by noticing that the algorithm gets the large gradient r once every three steps while for the two other steps it observes the gradient −1, which moves the algorithm in the wrong direction. The large gradient r is unable to counteract this effect since it is scaled down by a factor of almost r for the given value of $\\beta_2$, and hence the algorithm converges to 1 rather than −1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The AMSGRAD's trick:**\n",
    "\n",
    "To fix this convergence instability, Reddi et al. (2018) propose to tweak the bias correction for $v_t$. They suggest the following update:\n",
    "$$\\hat{v}_{t+1} = max (v_t, \\hat{v_t}) $$\n",
    "\n",
    "Reddi et al. (2018) takes the maximum instead of dividing $v_t$ by $1 - \\beta^{t}_2$ when updating the unbiased estimator $\\hat{v_t}$. The algorithm they have developed is called AMSGRAD.\n",
    "\n",
    "Let us implement it and test it to see how it compares to ADAM with the same hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMSGRAD (grad_func, Niter, alpha, beta_1, beta_2, epsilon): # grad_func denotes the gradient of our objective function\n",
    "    # Initialisation \n",
    "    x = 0\n",
    "    m_t = 0\n",
    "    v_t = 0\n",
    "    v_hat = 0\n",
    "    t = 0\n",
    "    x_prev = -10. #arbitrary value just to initialise the variable\n",
    "    \n",
    "    # Diving into the ADAM procedure\n",
    "    for t in range(Niter):\n",
    "        t+=1\n",
    "        g_t = grad_func(x, t, r) # gradient calculation\n",
    "        m_t = beta_1*m_t + (1.0-beta_1)*g_t # 1st moment estimate\n",
    "        v_t = beta_2*v_t + (1.0-beta_2)*(g_t*g_t) # 2nd moment estimate\n",
    "        m_hat = m_t/(1.0-(beta_1**t)) # 1st moment bias correction\n",
    "        v_hat = max(v_t, v_hat) # 2nd moment bias correction: THIS IS THE TRICK!!!!!\n",
    "        x_prev = x\n",
    "        x = projection(x - (alpha*m_hat)/(math.sqrt(v_hat)+epsilon)) # update\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(AMSGRAD(gradient_f, Niter, alpha, beta_1, beta_2, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, AMSGRAD converges to the optimum without the need to modify the hyper-parameters!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec5\"></a> 5. A FEW WORDS ON ADAMAX\n",
    "\n",
    "ADAMAX was introduced by Kingma and Lei Ba (2014) as an extension of ADAM and looks a little bit similar to AMSGRAD.\n",
    "\n",
    "Remember the ADAM's update rule:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t} $$\n",
    "\n",
    "We can see that the gradients are scaled inversely proportionally to a $L^2$ norm of their individual current and past gradients (this is hidden in $\\hat{v_t}$ and $v_t = \\beta_{2} v_{t} + (1 - \\beta_{2})g^{2}_t$).\n",
    "\n",
    "So why not generalising the $L^2$ norm-based update rule to a $L^p$ one? \n",
    "\n",
    "At first sight, one could argue that such variants become numerically unstable for large p. However, in the special case of the infinity norm ($p\\rightarrow \\infty$), we get a stable algorithm.\n",
    "\n",
    "\n",
    "For the sake of clarity, we denote $u_t$ the infinity norm-constrained $v_t$:\n",
    "\n",
    "$$u_t = \\beta^{\\infty}_{2}u_{t-1} + (1 - \\beta^{\\infty}_{2})|g_{t}|^{\\infty} = max(\\beta_2 \\cdot u_{t-1};|g_{t}|) $$\n",
    "\n",
    "\n",
    "In concrete terms, what we are doing is merely replacing $\\sqrt{\\hat{v}_{t}} + \\epsilon$ by $u_t$. We can now derive the ADAMAX's update rule:\n",
    "\n",
    "$$\\boxed{\\theta_{t+1} = \\theta_{t} - \\frac{\\alpha}{u_t} \\hat{m}_{t}} $$\n",
    "\n",
    "By tweaking ADAM, we get the ADAMAX algorithm:\n",
    "\n",
    "<img src=\"adamax.png\" width=\"800px\"></img>\n",
    "\n",
    "Kingma et Lei Ba (2014) recommend to use the following parameters:\n",
    "$\\alpha = 0.002$, $\\beta_1 = 0.9$ and  $\\beta_2 = 0.999$.\n",
    "\n",
    "Your turn! Could you take your cue from the ADAM algorithm we coded earlier on and implement ADAMAX? Give it a try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAMAX (grad_func, alpha, beta_1, beta_2): # grad_func denotes the gradient of our objective function\n",
    "    # Initialisation \n",
    "    theta_0 = 0\n",
    "    m_t = 0\n",
    "    u_t = 0\n",
    "    t = 0\n",
    "    theta_0_prev = -1.0 #arbitrary value just to initialise the variable\n",
    "    \n",
    "    # Diving into the ADAM procedure\n",
    "    while (theta_0 != theta_0_prev):\n",
    "        t+=1\n",
    "        g_t = grad_func(theta_0) # gradient calculation\n",
    "        m_t = beta_1*m_t + (1.0-beta_1)*g_t # 1st moment estimate\n",
    "        m_hat = m_t/(1.0-(beta_1**t)) # 1st moment bias correction\n",
    "        u_t = max(u_t*beta_2, abs(g_t)) \n",
    "        theta_0_prev = theta_0\n",
    "        theta_0 = theta_0 - (alpha*m_hat)/u_t # update\n",
    "    return(theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make use of the recommended parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.002\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test ADAMAX on a function we have already implemented: $x^{2} - 10x +4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999999999994\n"
     ]
    }
   ],
   "source": [
    "# The gradient of the objective function under study\n",
    "def gradient_obj_func (x):\n",
    "    return(2*x - 10)\n",
    "\n",
    "\n",
    "print(ADAMAX (gradient_obj_func, alpha, beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check whether ADAMAX works on the function for which ADAM fails. We need to make some slights changes to our ADAMAX algorithm to adapt to such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAMAX_bis (grad_func, Niter, alpha, beta_1, beta_): # grad_func denotes the gradient of our objective function\n",
    "    # Initialisation \n",
    "    x = 0\n",
    "    m_t = 0\n",
    "    u_t = 0\n",
    "    t = 0\n",
    "    x_prev = -10. #arbitrary value just to initialise the variable\n",
    "    \n",
    "    # Diving into the ADAM procedure\n",
    "    for t in range(Niter):\n",
    "        t+=1\n",
    "        g_t = grad_func(x, t, r) # gradient calculation\n",
    "        m_t = beta_1*m_t + (1.0-beta_1)*g_t # 1st moment estimate\n",
    "        m_hat = m_t/(1.0-(beta_1**t)) # 1st moment bias correction\n",
    "        v_hat = u_t = max(u_t*beta_2, abs(g_t))\n",
    "        x_prev = x\n",
    "        x = projection(x - - (alpha*m_hat)/u_t) # update\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyperparameters\n",
    "beta_1 = 0.\n",
    "beta_2 = float(1/(1+r*r))\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.998\n"
     ]
    }
   ],
   "source": [
    "print(ADAMAX_bis(gradient_f, Niter, alpha, beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we use parameters that are different from the recommended ones, ADAMAX does work whereas ADAM does not!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "In this notebook, we have discovered ADAM, a recent optimisation algorithm that can prove useful when training neural networks (i.e. learning the weights of the network).\n",
    "\n",
    "The key takeaways are:\n",
    "\n",
    "- ADAM combines the advantages of Adagrad and RMSProp, two other optimisation procedures\n",
    "- ADAM is appropriate to problems with large datasets, sparse gradients and/or noisy functions.\n",
    "- ADAM can be used online\n",
    "- ... however if you change the recommended hyper-parameters, ADAM fails to converge for some functions. This can be fixed by tweaking the ADAM's update rule (AMSGRAD, ADAMAX) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16pt; line-height:19pt; font-weight:bold; text-align:center;\">Hope you've learnt some new stuffs! Thank you for reading :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Diederik P. Kingma and Jimmy Lei Ba. Adam : A method for stochastic optimization. 2014. arXiv:1412.6980v9](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "\n",
    "[John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "\n",
    "\n",
    "\n",
    "[Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA- neural networks for machine learning, 4(2), 26–31, 2012.](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
    "\n",
    "\n",
    "\n",
    "[Nitish Shirish Keskar, Richard Socher. Improving Generalization Performance by Switching from Adam to SGD. 2017 arXiv:1712.07628v1](https://arxiv.org/abs/1712.07628)\n",
    "\n",
    "\n",
    "[Sashank J. Reddi, Satyen Kale, Sanjiv Kumar. On the Convergence of Adam and Beyond. 2018.](https://openreview.net/forum?id=ryQu7f-RZ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
